{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0bd2663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is just to increase my Jupyter Notebook size to fill my screen.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4098ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary Pre-requisites\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import codecs\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stopList = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4a8b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a method to open the file such that it can be read by Python\n",
    "def readFile(filename):\n",
    "    with open(filename, \"r\", encoding='mac_roman') as f:\n",
    "        content = f.read()\n",
    "    f.close()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75330310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our directory and opening the corpus\n",
    "cranCorpus = \"C:\\\\Users\\\\zjshe\\\\Documents\\\\01. ECU MS CS\\\\02. Spring 2022\\\\02. CSCI 6030\\\\03. Assignments\\\\02. Programming Assignments\\\\03-programming-assignment\\\\cranfield-corpus\\\\cran.all.1400\"\n",
    "cranQRY = \"C:\\\\Users\\\\zjshe\\\\Documents\\\\01. ECU MS CS\\\\02. Spring 2022\\\\02. CSCI 6030\\\\03. Assignments\\\\02. Programming Assignments\\\\03-programming-assignment\\\\cranfield-corpus\\\\cran.qry\"\n",
    "cranRel = \"C:\\\\Users\\\\zjshe\\\\Documents\\\\01. ECU MS CS\\\\02. Spring 2022\\\\02. CSCI 6030\\\\03. Assignments\\\\02. Programming Assignments\\\\03-programming-assignment\\\\cranfield-corpus\\cranqrel\"\n",
    "\n",
    "# Defining Porter Stemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8189fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a variable to identify the index delimiter '.I'\n",
    "docID = re.compile('.\\.I.')\n",
    "\n",
    "# Method to open the file and retrieve each of the document indexes in the Cran Corpus\n",
    "def getData(filePath, iD):\n",
    "    with open (filePath, 'r') as f:\n",
    "        text = f.read().replace('\\n',\" \")\n",
    "        lines = re.split(iD, text)\n",
    "        lines.pop(0)\n",
    "    return lines\n",
    "\n",
    "# Retreiving document IDs from the Cran Corpus and Cran Query and storing them each in a variable. \n",
    "txtList = getData(cranCorpus, docID)\n",
    "qryList = getData(cranQRY, docID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "beea7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for normalization and stemming of Corpus\n",
    "def normalizeTxt(dirtyTxt):\n",
    "    # Converting text to lower case\n",
    "        lines = dirtyTxt.lower()\n",
    "    \n",
    "    # Removing punctuations\n",
    "        lines = re.sub('[^A-Za-z]+', ' ', lines)\n",
    "    \n",
    "    # Removing whitespaces to generate tokens\n",
    "        tokens = line.split()\n",
    "        \n",
    "    # removing stop words from the tokens\n",
    "        cleanTokens = [word for word in tokens if word not in stopList]\n",
    "    \n",
    "    # Tokenization\n",
    "        tokenList = word_tokenize(dirtyTxt)\n",
    "    \n",
    "    # Checking to verify we only get alphabetic characters\n",
    "        tokenList = [word for word in tokenList if word.isalpha()]\n",
    "    \n",
    "    # Stemming\n",
    "        stemedList = [ps.stem(word) for word in tokenList]\n",
    "        \n",
    "    # checking for stopwords again\n",
    "        twiceCleanedTokens = [word for word in stemedList if word not in stop_list]\n",
    "        \n",
    "        return twiceCleanedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3279e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the parser and dictionary based on the corpus delimiters (excluding Index)\n",
    "textPars = re.compile('\\.[A,B,T,W]')\n",
    "\n",
    "# Creating the dictionary to store the corpus information based\n",
    "txtData = defaultdict(dict)\n",
    "\n",
    "# Method to parse the text and store the ID, Title, Author, Publication Information, and Text Body based on the delimiters \".I, .T, .A, .B, .W\"\n",
    "# Performs normalization on Title and Text Fields\n",
    "\n",
    "for line in txtList:\n",
    "    entries = re.split(textPars,line)\n",
    "    id = entries[0].strip()\n",
    "    title = entries[1]\n",
    "    author = entries[2]\n",
    "    publicationInfo = entries[3]\n",
    "    text = entries[4]\n",
    "    txtData[id]['title'] = normalizeTxt(title)\n",
    "    txtData[id]['author'] = author\n",
    "    txtData[id]['publicationInfo'] = publicationInfo\n",
    "    txtData[id]['text'] = normalizeTxt(text)\n",
    "    \n",
    "    # Uncomment to test integerity of normalize method on either text or title.\n",
    "    # print(txtData[id]['text'])\n",
    "    # print(txtData[id]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fa8ab4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the queries posted on the corpus\n",
    "qryPars = re.compile('\\.[W]')\n",
    "qryData = defaultdict(dict)\n",
    " \n",
    "for n in range(0,len(qryList)-1):\n",
    "  line = qryList[n+1]\n",
    "  _ , question = re.split(qryPars,line)\n",
    "  qryData[n+1]['question'] = question\n",
    "\n",
    "# print(qryData) \n",
    "\n",
    "# Need to figure out why it doesn't start at question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3692dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of dictionary to store relevence assessment\n",
    "cranRelTxt = open(cranRel)\n",
    "cranNumPy = np.loadtxt(cranRelTxt, dtype = int)\n",
    "cranRelDic = defaultdict(list)\n",
    "for row in cranNumPy:\n",
    "    cranRelDic[row[0]].append(tuple(row[1:]))\n",
    "#     print(cranRelDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c86546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work completed so far\n",
    "# 1) Created a normalize method\n",
    "# 2) Created Dictionary of doc ID, non-normalized author and publication Info, and normalized title and text\n",
    "# 3) Created Dictionary for each query in the provided query doc.\n",
    "# 4) Created Dictionary to store relevence assessments from provided relevence document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2baeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining work to be completed\n",
    "# 1) Create a dicitionary of all tokenized words and their frequency\n",
    "#    a)  Need dictionary for text\n",
    "#    b)  Need diciontary for title\n",
    "# 2) Calculate DF values for each term in the vocabulary\n",
    "# 3) Calcualte tf-idf vlaues for each term in the vocabulary\n",
    "# 4) Form document vectors using tf-idf values\n",
    "# 5) Develop user interface which allows user to boost vlaues\n",
    "# 6) Compute Compute the similarity of the query with the corpus documents to produce top k results\n",
    "# 7) Show titles of ranked documetns that are relvent to query. (May also use abstract for this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
