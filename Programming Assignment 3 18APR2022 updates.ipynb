{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0bd2663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is just to increase my Jupyter Notebook size to fill my screen.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4098ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary Pre-requisites\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stopList = stopwords.words('english')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d4a8b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a method to open the file such that it can be read by Python\n",
    "def readFile(filename):\n",
    "    with open(filename, \"r\", encoding='mac_roman') as f:\n",
    "        content = f.read()\n",
    "    f.close()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "75330310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input filepath of cran corpus as located on YOUR LOCAL MACHINE\n",
      "Please input filepath of cran qry as located on YOUR LOCAL MACHINE\n",
      "Please input filepath of cran qrel as located on YOUR LOCAL MACHINE\n"
     ]
    }
   ],
   "source": [
    "# Defining our directory and opening the corpus\n",
    "\n",
    "print('Please input filepath of cran corpus as located on YOUR LOCAL MACHINE')\n",
    "#cranCorpus=\"C:\\\\Users\\\\J\\\\Desktop\\\\Data Science\\\\Spring 2022\\\\DASC 6030\\\\cranfield-corpus\\\\cran.all.1400\"\n",
    "cranCorpus=input()\n",
    "#\"C:\\\\Users\\\\J\\\\Desktop\\\\Data Science\\\\Spring 2022\\\\DASC 6030\\\\cranfield-corpus\\\\cran.all.1400\"\n",
    "print('Please input filepath of cran qry as located on YOUR LOCAL MACHINE')\n",
    "#cranQRY=\"C:\\\\Users\\\\J\\\\Desktop\\\\Data Science\\\\Spring 2022\\\\DASC 6030\\\\cranfield-corpus\\\\cran.qry\"\n",
    "cranQRY=input()\n",
    "#\"C:\\\\Users\\\\J\\\\Desktop\\\\Data Science\\\\Spring 2022\\\\DASC 6030\\\\cranfield-corpus\\\\cran.qry\"\n",
    "print('Please input filepath of cran qrel as located on YOUR LOCAL MACHINE')\n",
    "#cranRel=\"C:\\\\Users\\\\J\\\\Desktop\\\\Data Science\\\\Spring 2022\\\\DASC 6030\\\\cranfield-corpus\\\\cranqrel\"\n",
    "cranRel=input()\n",
    "#\"C:\\\\Users\\\\J\\\\Desktop\\\\Data Science\\\\Spring 2022\\\\DASC 6030\\\\cranfield-corpus\\\\cranqrel\"\n",
    "\n",
    "\n",
    "# Defining Porter Stemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c8189fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a variable to identify the index delimiter '.I'\n",
    "docID = re.compile('.\\.I.')\n",
    "\n",
    "# Method to open the file and retrieve each of the document indexes in the Cran Corpus\n",
    "def getData(filePath, iD):\n",
    "    with open (filePath, 'r') as f:\n",
    "        text = f.read().replace('\\n',\" \")\n",
    "        lines = re.split(iD, text)\n",
    "        lines.pop(0)\n",
    "    return lines\n",
    "\n",
    "# Retreiving document IDs from the Cran Corpus and Cran Query and storing them each in a variable. \n",
    "txtList = getData(cranCorpus, docID)\n",
    "qryList = getData(cranQRY, docID)\n",
    "\n",
    "#print(len(txtList))\n",
    "#should return 1400\n",
    "#print(len(qryList))\n",
    "#should return 225\n",
    "\n",
    "numDoc = (len(txtList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "beea7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for normalization and stemming of Corpus\n",
    "def normalizeTxt(dirtyTxt):\n",
    "    # Converting text to lower case\n",
    "        lines = dirtyTxt.lower()\n",
    "    \n",
    "    # Removing punctuations\n",
    "        lines = re.sub('[^A-Za-z]+', ' ', lines)\n",
    "    \n",
    "    # Removing whitespaces to generate tokens\n",
    "        tokens = line.split()\n",
    "        \n",
    "    # removing stop words from the tokens\n",
    "        cleanTokens = [word for word in tokens if word not in stopList]\n",
    "    \n",
    "    # Tokenization\n",
    "        tokenList = word_tokenize(dirtyTxt)\n",
    "    \n",
    "    # Checking to verify we only get alphabetic characters\n",
    "        tokenList = [word for word in tokenList if word.isalpha()]\n",
    "    \n",
    "    # Stemming\n",
    "        stemedList = [ps.stem(word) for word in tokenList]\n",
    "        \n",
    "    # checking for stopwords again\n",
    "        twiceCleanedTokens = [word for word in stemedList if word not in stopList]\n",
    "        \n",
    "        return twiceCleanedTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3279e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the parser and dictionary based on the corpus delimiters (excluding Index)\n",
    "textPars = re.compile('\\.[A,B,T,W]')\n",
    "\n",
    "# Creating the dictionary to store the corpus information based\n",
    "txtData = defaultdict(dict)\n",
    "\n",
    "\n",
    "# Method to parse the text and store the ID, Title, Author, Publication Information, and Text Body based on the delimiters \".I, .T, .A, .B, .W\"\n",
    "# Performs normalization on Title and Text Fields\n",
    "\n",
    "for line in txtList:\n",
    "    entries = re.split(textPars,line)\n",
    "    id = entries[0].strip()\n",
    "    title = entries[1]\n",
    "    author = entries[2]\n",
    "    publicationInfo = entries[3]\n",
    "    text = entries[4]\n",
    "    txtData[id]['title'] = normalizeTxt(title)\n",
    "    txtData[id]['author'] = author\n",
    "    txtData[id]['publicationInfo'] = publicationInfo\n",
    "    txtData[id]['text'] = normalizeTxt(text)\n",
    "    \n",
    "    # Uncomment to test integerity of normalize method on either text or title.\n",
    "    \n",
    "    #print(id, txtData[id]['text'])\n",
    "    #print(txtData[id]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1dc7b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the parser and dictionary based on the query delimiters (excluding Index)\n",
    "qryPars = re.compile('\\.[I,W]')\n",
    "\n",
    "# Creating the dictionary to store the corpus information based\n",
    "qryData = defaultdict(dict)\n",
    "\n",
    "\n",
    "# Method to parse the text and store the ID and query based on the delimiters \".I, .W\"\n",
    "# Performs normalization on query Field\n",
    "\n",
    "for line in qryList:\n",
    "    entries = re.split(qryPars,line)\n",
    "    id = entries[0].strip()\n",
    "    qryText = entries[1]\n",
    "    qryData[id]['qryText'] = normalizeTxt(qryText)\n",
    "    \n",
    "    # Uncomment to test integerity of normalize method on query\n",
    "    \n",
    "    #print(id, qryData[id]['qryText'])\n",
    "    #print(qryData[id]['qryText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fa8ab4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the queries posted on the corpus\n",
    "qryPars = re.compile('\\.[W]')\n",
    "qryData = defaultdict(dict)\n",
    " \n",
    "for n in range(0,len(qryList)-1):\n",
    "  line = qryList[n+1]\n",
    "  _ , question = re.split(qryPars,line)\n",
    "  qryData[n+1]['question'] = question\n",
    "\n",
    "#To print all queries\n",
    "  # print(qryData) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3692dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of dictionary to store relevence assessment\n",
    "cranRelTxt = open(cranRel)\n",
    "cranNumPy = np.loadtxt(cranRelTxt, dtype = int)\n",
    "cranRelDic = defaultdict(list)\n",
    "for row in cranNumPy:\n",
    "    cranRelDic[row[0]].append(tuple(row[1:]))\n",
    "    #print(cranRelDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0de241b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cran_rel_data = open(cranRel)\n",
    "cran_np = np.loadtxt(cran_rel_data, dtype=int)\n",
    " \n",
    "cran_rel_rat = defaultdict(list)\n",
    "for row in cran_np:\n",
    "    cran_rel_rat[row[0]].append(tuple(row[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "86c86546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#successfully creates list of text tokens\n",
    "dictTokens = []\n",
    "for id in txtData:\n",
    "    dictTokens.append(txtData[id]['text'])\n",
    "#print(dictTokens)\n",
    "\n",
    "#successfully creates list of title tokens\n",
    "titleTokens = []\n",
    "for id in txtData:\n",
    "    titleTokens.append(txtData[id]['title'])\n",
    "#print(titleTokens)\n",
    "\n",
    "#successfully creates list of query tokens\n",
    "qryTokens = []\n",
    "for id in qryData:\n",
    "    qryTokens.append(qryData[id]['qryText'])\n",
    "#print(qryTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ceb403d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of key-value pairs where tokens are keys and their occurrence in the corpus the value for text\n",
    "textDF = {}\n",
    "for id in txtData:\n",
    "    tokens = txtData[id]['text']\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            # add token as key and doc number as value is chained\n",
    "            textDF[w].add(id)\n",
    "        except:\n",
    "            # to handle when a new token is encountered\n",
    "            textDF[w] = {id}\n",
    "\n",
    "# convert to number of occurrences of the token from list of documents where token occurs for text\n",
    "for id in textDF:\n",
    "    textDF[id] = len(textDF[id])\n",
    "\n",
    "#print(textDF)\n",
    "\n",
    "# create a dictionary of key-value pairs where tokens are keys and their occurrence in the corpus the value for title\n",
    "titleDF = {}\n",
    "for id in txtData:\n",
    "    tokens = txtData[id]['title']\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            # add token as key and doc number as value is chained\n",
    "            titleDF[w].add(id)\n",
    "        except:\n",
    "            # to handle when a new token is encountered\n",
    "            titleDF[w] = {id}\n",
    "\n",
    "# convert to number of occurrences of the token from list of documents where token occurs for text\n",
    "for id in titleDF:\n",
    "    titleDF[id] = len(titleDF[id])\n",
    "\n",
    "#print(titleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1629f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of unique words in the corpus text\n",
    "textVocabSize = len(textDF)\n",
    "#print(textVocabSize)\n",
    "\n",
    "# create vocabulary list of all unique words in text\n",
    "textVocab = [term for term in textDF]\n",
    "#print(textVocab)\n",
    "\n",
    "# count number of unique words in the corpus title\n",
    "titleVocabSize = len(titleDF)\n",
    "#print(titleVocabSize)\n",
    "\n",
    "# create vocabulary list of all unique words in title\n",
    "titleVocab = [term for term in titleDF]\n",
    "#print(titleVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b3229942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary to store tf-idf values for each term in the vocabulary for text\n",
    "doc = 0\n",
    "textTfIdf = {}\n",
    "\n",
    "for id in txtData:\n",
    "    tokens = txtData[id]['text']\n",
    "\n",
    "    # counter object to efficiently count number of occurrence of a term in a particular document for text\n",
    "    counter = Counter(tokens)\n",
    "    token_count = len(tokens)\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        # counting occurence of term in object using counter object\n",
    "        tf = counter[token] / token_count\n",
    "        # retrieving df values from DF dictionary\n",
    "        df = textDF[token] if token in textVocab else 0\n",
    "\n",
    "        # adding 1 to numerator & denominator to avoid divide by 0 error\n",
    "        idf = np.log((numDoc + 1) / (df + 1))\n",
    "\n",
    "        textTfIdf[doc, token] = tf * idf\n",
    "\n",
    "    doc += 1\n",
    "\n",
    "#print(textTfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bcae1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary to store tf-idf values for each term in the vocabulary for title\n",
    "doc = 0\n",
    "titleTfIdf = {}\n",
    "\n",
    "for id in txtData:\n",
    "    tokens = txtData[id]['title']\n",
    "\n",
    "    # counter object to efficiently count number of occurrence of a term in a particular document for title\n",
    "    counter = Counter(tokens)\n",
    "    token_count = len(tokens)\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        # counting occurence of term in object using counter object\n",
    "        tf = counter[token] / token_count\n",
    "        # retrieving df values from DF dictionary\n",
    "        df = titleDF[token] if token in titleVocab else 0\n",
    "\n",
    "        # adding 1 to numerator & denominator to avoid divide by 0 error\n",
    "        idf = np.log((numDoc + 1) / (df + 1))\n",
    "\n",
    "        titleTfIdf[doc, token] = tf * idf\n",
    "\n",
    "    doc += 1\n",
    "\n",
    "#print(titleTfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "13adc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing empty vector of vocabulary size\n",
    "D = np.zeros((numDoc, textVocabSize))\n",
    "\n",
    "#creating vector of tf-idf values\n",
    "for i in textTfIdf:\n",
    "    ind = textVocab.index(i[1])\n",
    "    D[i[0]][ind] = textTfIdf[i]\n",
    "\n",
    "#confirm number of vectors and vector preview\n",
    "#print(len(D))\n",
    "#print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db1e926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "    \"\"\"To create a vector (with repsect to the vocabulary) of the tokens passed as input\n",
    "    \n",
    "    Arguments:\n",
    "        tokens {list} -- list of tokens to be converted\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray -- vector of tokens\n",
    "    \"\"\"\n",
    "    Q = np.zeros((len(textVocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    tokenCount = len(tokens)\n",
    "\n",
    "    qryWeights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/tokenCount\n",
    "        df = textDF[token] if token in textVocab else 0\n",
    "        idf = np.log((numDoc+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = textVocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "074e83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    \"\"\"To calculate cosine similarity between 2 vectors.\n",
    "    \n",
    "    Arguments:\n",
    "        x {numpy.ndarray} -- vector 1\n",
    "        y {numpy.ndarray} -- vector 2\n",
    "    \n",
    "    Returns:\n",
    "        numpy.float64 -- cosine similarity between vector 1 & vector 2\n",
    "    \"\"\"\n",
    "    cos_sim = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "    \n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "49f13710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    \"\"\"To determine a ranked list of top k documents in descending order of their\n",
    "    cosine similarity with the query\n",
    "    \n",
    "    Arguments:\n",
    "        k {integer} -- top k documents to retrieve from \n",
    "        query {string} -- query whose cosine similarity is to be computed with the corpus\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray -- list of top k cosine similarities between query and corpus of documents\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = query.split()\n",
    "      \n",
    "    d_cosines = []\n",
    "    \n",
    "    # vectorize the input query tokens\n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    if k == 0:\n",
    "        # k=0 to retrieve all documents in descending order\n",
    "        out = np.array(d_cosines).argsort()[::-1]\n",
    "        \n",
    "    else:\n",
    "        # to retrieve the top k documents in descending order    \n",
    "        out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "06de942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_docs(k):\n",
    "    \"\"\"To generate a ranked list of k top documents in descending order of their cosine similarity \n",
    "    calculated against the queries. Output is a list of (query id, document id) pairs.\n",
    "    \n",
    "    If k=0 is given as input then list of all documnets in descending order is returned.\n",
    "    \n",
    "    Arguments:\n",
    "        k {integer} -- number of top documents to be retrieved\n",
    "    \n",
    "    Returns:\n",
    "        list -- list of documents in descending order of their cosine similarity\n",
    "    \"\"\"\n",
    "    cos_sims = []\n",
    "    for i in range(len(qryList)):\n",
    "        cs = [i, cosine_similarity(k, qryList[i])]\n",
    "        cos_sims.append(cs)\n",
    "        \n",
    "    return cos_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2ef65329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get list of all documents\n",
    "#May result in runtime error on local machine!\n",
    "#May resolve with from scipy.special import logsumexp ...?\n",
    "#cos_sim = np.dot(x, y) / (np.linalg.norm.logsumexp(x)*np.linalg.norm.logsumexp(y)) ...?\n",
    "\n",
    "#no_of_top=0\n",
    "#list_of_docs(no_of_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "85a58d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieving relevance values from cranqrel\n",
    "# Creating the parser and dictionary based on the cranqrel delimiters\n",
    "textPars = re.compile('\\.[A,B,T,W]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work completed so far\n",
    "# 1) Created a normalize method\n",
    "# 2) Created Dictionary of doc ID, non-normalized author and publication Info, and normalized title and text\n",
    "# 3) Created Dictionary for each query in the provided query doc.\n",
    "# 4) Created Dictionary to store relevence assessments from provided relevence document\n",
    "# 5) Create a dicitionary of all tokenized words and their frequency\n",
    "#    a)  Need dictionary for text\n",
    "#    b)  Need diciontary for title\n",
    "# 6) Calculate DF values for each term in the vocabulary\n",
    "# 7) Calcualte tf-idf vlaues for each term in the vocabulary\n",
    "# 8) Form document vectors using tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2baeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining work to be completed\n",
    "\n",
    "# 5) Develop user interface which allows user to boost vlaues\n",
    "# 6) Compute Compute the similarity of the query with the corpus documents to produce top k results\n",
    "# 7) Show titles of ranked documetns that are relvent to query. (May also use abstract for this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
