{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'to', 'the', 'crazi', 'one', 'the', 'misfit', 'the', 'rebel', 'the', 'troublemak', 'the', 'round', 'peg', 'in', 'the', 'squar', 'hole', 'the', 'one', 'who', 'see', 'thing', 'differ', 'theyr', 'not', 'fond', 'of', 'rule', 'you', 'can', 'quot', 'them', 'disagre', 'with', 'them', 'glorifi', 'or', 'vilifi', 'them', 'but', 'the', 'onli', 'thing', 'you', 'cant', 'do', 'is', 'ignor', 'them', 'becaus', 'they', 'chang', 'thing', 'they', 'push', 'the', 'human', 'race', 'forward', 'and', 'while', 'some', 'may', 'see', 'them', 'as', 'the', 'crazi', 'one', 'we', 'see', 'geniu', 'becaus', 'the', 'one', 'who', 'are', 'crazi', 'enough', 'to', 'think', 'that', 'they', 'can', 'chang', 'the', 'world', 'are', 'the', 'one', 'who', 'do']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/seymonegugneja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently — they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think that they can change the world, are the ones who do.\"\n",
    "#get rid of header/footer\n",
    "#lowercase\n",
    "text_lower = text.lower()\n",
    "cleanPunct = (string.punctuation).replace(\"'\", \"\")\n",
    "tokens = text_lower.split()\n",
    "\n",
    "#to remove punctuation\n",
    "noPuncString = re.sub(r'[^\\w\\s]','', text_lower)\n",
    "# print(noPuncString)\n",
    "\n",
    "# convert string to list of words\n",
    "lst_string = [noPuncString][0].split()\n",
    "\n",
    "#use the Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "stemmedList = []\n",
    "for word in lst_string:\n",
    "    stemmedList.append(ps.stem(word))\n",
    "\n",
    "print(stemmedList)\n",
    "    \n",
    "#NLTK\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     lemmaList = []\n",
    "#     for word in nltk_stemedList:\n",
    "#         nltk_lemmaList.append(wordnet_lemmatizer.lemmatize(word))\n",
    "\n",
    "#nltk\n",
    "# nltk_tokenList = word_tokenize(noPuncString)\n",
    "# print(nltk_tokenList)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/seymonegugneja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "dir_path = \"/Users/seymonegugneja/Desktop/4130/prog-assg-2/corpus\"\n",
    "\n",
    "def readFile(filename):\n",
    "    with open(filename, \"r\", encoding='mac_roman') as f:\n",
    "        dirtyTxt = f.read()\n",
    " \n",
    "    f.close()\n",
    "\n",
    "    return dirtyTxt\n",
    "\n",
    "def normalizeTxt(dirtyTxt):\n",
    "    #nltk\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for root, subdirectories, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "\n",
    "    #             docID += 1\n",
    "\n",
    "                filename = os.fsdecode(file)\n",
    "                if(filename.endswith(\".txt\")):\n",
    "                    currFile = root + \"/\" + filename\n",
    "#                     print(currFile)\n",
    "                    tmpFile = readFile(currFile)\n",
    "#                     print(tmpFile)\n",
    "    #                 cleanTxt = normalizeTxt(tmpFile)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
