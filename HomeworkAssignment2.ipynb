{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6922f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zjshe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zjshe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preliminaries\n",
    "import os\n",
    "from os import path\n",
    "from os.path import isfile, join\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8e4047",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For proximity query enter 6. \n",
      "For phrase query enter 9 \n",
      "To Quit enter 0 \n",
      "0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zjshe\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#File path used to read Corpus\n",
    "dir_path = '/Users/zjshe/Documents/01. ECU MS CS/02. Spring 2022/02. CSCI 6030/03. Assignments/02. Programming Assignments/02-programming-assignment/All Files'\n",
    "path = \"C:\\\\Users\\\\zjshe\\\\Documents\\\\01. ECU MS CS\\\\02. Spring 2022\\\\02. CSCI 6030\\\\03. Assignments\\\\02. Programming Assignments\\\\02-programming-assignment\\\\corpus\\\\Good\"\n",
    "#Defining PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#List to store words\n",
    "mylist = []\n",
    "\n",
    "#Method for opening, reading, and encoding files from Corpus\n",
    "def readFile(filename):\n",
    "    with open(filename, \"r\", encoding='mac_roman') as f:\n",
    "        dirtyTxt = f.read()\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    return dirtyTxt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Method for normalization and stemming of Corpus\n",
    "def normalizeTxt(dirtyTxt):\n",
    "    #text_lower = text.lower()\n",
    "    \n",
    "    #Tokenization\n",
    "        tokenList = word_tokenize(dirtyTxt)\n",
    "    \n",
    "        tokenList = [word for word in tokenList if word.isalpha()]\n",
    "    \n",
    "    \n",
    "        #Stemming\n",
    "        stemedList = [ps.stem(word) for word in tokenList]\n",
    "        return stemedList\n",
    "        \n",
    "\n",
    "#Method for building Positional Index\n",
    "def posInd(path):\n",
    "\n",
    "    posIndex = {}\n",
    "    docDict = {}   \n",
    "    docID = 0\n",
    "    for root, subdirectories, files in os.walk(path):\n",
    "        for file in files:\n",
    "\n",
    "            filename = os.fsdecode(file)\n",
    "            if(filename.endswith(\".txt\")):\n",
    "                file = root + '\\\\' + filename\n",
    "                tempTxt = readFile(file)\n",
    "                cleanText = normalizeTxt(tempTxt)\n",
    "                docDict[docID] = filename\n",
    "                \n",
    "                for position, word in enumerate(cleanText):\n",
    "                    if word in posIndex:\n",
    "\n",
    "                        posIndex[word][0] = posIndex[word][0] + 1\n",
    "\n",
    "                        if docID in posIndex[word][1]:\n",
    "                            posIndex[word][1][docID].append(position)\n",
    "                             \n",
    "                        else:\n",
    "                            posIndex[word][1][docID] = [position]\n",
    "                    else:\n",
    "                         \n",
    "                      \n",
    "                        posIndex[word] = []\n",
    "                        posIndex[word].append(1)\n",
    "                        posIndex[word].append({})     \n",
    "                        posIndex[word][1][docID] = [position]\n",
    "            docID += 1\n",
    "\n",
    "    return posIndex, docDict\n",
    " \n",
    "#Compares two lists and takes the insterection of them\n",
    "def intersect(list1, list2):\n",
    " \n",
    "    \n",
    "    temp = set(list2)\n",
    "    lst = [value for value in list1 if value in temp]\n",
    "    return lst\n",
    "\n",
    "#Takes inputs and then performs a query\n",
    "def PhraseQuery(queryWords, posIndex, i = 0):\n",
    "\n",
    "    mainIndex = []\n",
    "\n",
    "    for word in queryWords:\n",
    "        try:\n",
    "            mainIndex.append(posIndex[word])\n",
    "        except KeyError:\n",
    "            print(\"Word not found\")\n",
    "            return\n",
    "\n",
    "\n",
    "    intersectResults = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for words in mainIndex:\n",
    "        if count == 0:\n",
    "            intersectResults = mainIndex[count][1].keys()\n",
    "        else:\n",
    "            intersectResults = intersect(intersectResults, mainIndex[count][1].keys())\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    \n",
    "\n",
    "    if i == 0:\n",
    "        queryResult = []\n",
    "        for docIDs in intersectResults:\n",
    "            positionCheck = []\n",
    "            for index, words in enumerate(mainIndex):\n",
    "                tempInd = []\n",
    "                tempInd = mainIndex[index][1].get(docIDs)\n",
    "                    \n",
    "                adjustedIndex = [x - index for x in tempInd]\n",
    "\n",
    "                if index == 0:\n",
    "                    positionCheck = adjustedIndex\n",
    "                else: \n",
    "                    positionCheck = intersect(positionCheck, adjustedIndex)\n",
    "\n",
    "            if positionCheck:\n",
    "                queryResult.append(docIDs)\n",
    "    else:\n",
    "        queryResult = []\n",
    "        for docIDs in intersectResults:\n",
    "            positions1 = []\n",
    "            positions2 = []\n",
    "            for index, words in enumerate(mainIndex):\n",
    "                tempIndex = []\n",
    "                tempIndex = mainIndex[index][1].get(docIDs)\n",
    "                kAdjustInd = [x - i for x in tempIndex]\n",
    "\n",
    "                if index == 0:\n",
    "                    positions1 = tempIndex\n",
    "                else:\n",
    "                    positions2 = kAdjustInd\n",
    "\n",
    "                    for x in positions1:\n",
    "                        for y in positions2:\n",
    "                            if y <= x:\n",
    "                                tempVal = x - y\n",
    "                                if abs(tempVal) <= i:\n",
    "                                    if docIDs not in queryResult:\n",
    "                                        queryResult.append(docIDs)\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "    return queryResult\n",
    "        \n",
    "#User interface that allows the user to select query type, enter query and assocaited parameters and provides results.     \n",
    "def main():\n",
    "    \n",
    "    posIndex = {}\n",
    "    docDict = {}\n",
    "    posIndex, docDict = posInd(path)\n",
    "    \n",
    "    while(True):\n",
    "        userInput = input(\"For proximity query enter 6. \\nFor phrase query enter 9 \\nTo Quit enter 0 \\n\")\n",
    "        if int(userInput) == 6:\n",
    "            inpt = input(\"Enter phrase to search: \")\n",
    "            results = PhraseQuery(normalizeTxt(inpt), posIndex)\n",
    "            if results:\n",
    "                for x in results:\n",
    "                    print(docDict.get(x))\n",
    "                    \n",
    "        elif int(userInput) == 9:\n",
    "            inpt1 = input(\"Enter tokens to search: \")\n",
    "            inpt2 = int(input(\"Enter distance between values as a whole numbers: \"))\n",
    "            results = PhraseQuery(normalizeTxt(inpt1), posIndex, int(inpt2))\n",
    "            if results:\n",
    "                for x in results:\n",
    "                    print(docDict.get(x))\n",
    "            \n",
    "        elif int(userInput) == 0:\n",
    "            exit()\n",
    "                          \n",
    "        else:\n",
    "            exit()\n",
    "\n",
    "                    \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3745c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
